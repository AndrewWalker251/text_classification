{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data import Field,LabelField\n",
    "from torchtext.data import Example\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchtext import data\n",
    "import time\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Text Classification for Hackathon.\n",
    "\n",
    "https://www.machinehack.com/course/classifying-movie-scripts-predict-the-movie-genre/\n",
    "\n",
    "The problem gives you the whole text from a movie script and you need to classify it into one of the genres. \n",
    "\n",
    "\n",
    "### Summarise the approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful references:\n",
    "\n",
    "This is a starter for looking at using torchtext and then inputting the result into a LSTM. The problem here is that I think our corpus of text is too long for LSTM.\n",
    "\n",
    "https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm\n",
    "\n",
    "\n",
    "This second approach uses an embeddingbag - which averages out the embedding values. This is better for our application as we can compare the approach on scripts of different lengths. It also looks at using ngrams.\n",
    "\n",
    "So we take all the words (each has a vector - of say 200 length) and we average it for all words. That will give us one vector that's then the same length for all movie scripts. You must remove a lot of information in that averaging. \n",
    "\n",
    "https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>\\t\\t\\tCrouching Tiger, Hidden Dragon\\n\\n\\t\\t\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>\"MUMFO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>MAX PAYNE\\n\\n          ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>SLUMDOG MILLIONAIRE\\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>15</td>\n",
       "      <td>THE OTHER BOLEYN GIR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>GET CARTER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>CRAZY, STUPID, LOV...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>TRISTAN + ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "      <td>BADLANDS\\n\\nby Terence Malick\\n\\nFinal Version...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>Rev.  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>THE CURIOUS CASE OF BENJAMIN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>\\t\\t\\t\\t\\tTHE MATRIX\\n\\n\\t\\t\\t\\t\\tWritten by\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>6</td>\n",
       "      <td>DRIVE\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6</td>\n",
       "      <td>\"The Truman Show\", early, by Andrew M. Niccol\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>DETROIT RO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>DEVIL IN A BLUE DRESS\\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>Spider-Man Script\\n\\n&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>\"LIF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>6</td>\n",
       "      <td>9\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5</td>\n",
       "      <td>PUBLIC ENEMIES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>19</td>\n",
       "      <td>THE ROOMMATE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>6</td>\n",
       "      <td>MY GIRL\\n\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4</td>\n",
       "      <td>TWO FOR TH...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>19</td>\n",
       "      <td>HEAT\\n\\n           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19</td>\n",
       "      <td>\"DIE HARD\"\\n\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>15</td>\n",
       "      <td>THE SHIPPING NEWS\\n\\n    ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>19</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2</td>\n",
       "      <td>MONKEYBONE\\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1948</th>\n",
       "      <td>15</td>\n",
       "      <td>MARLEY &amp; ME\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>15</td>\n",
       "      <td>IT'S COMPLICATED\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>14</td>\n",
       "      <td>INCEPTION\\n\\n         ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>0</td>\n",
       "      <td>Air Force One\\n\\n&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>6</td>\n",
       "      <td>CLASH OF THE TITAN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>5</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1954</th>\n",
       "      <td>14</td>\n",
       "      <td>A FEW GO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1955</th>\n",
       "      <td>1</td>\n",
       "      <td>THE LORD OF THE RI...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1956</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Dark Star\", short film script, by John Carpen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1957</th>\n",
       "      <td>4</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;\\n\\n&lt;b&gt;/*\\n\\n&lt;/b&gt;Break-out-of-f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1958</th>\n",
       "      <td>6</td>\n",
       "      <td>\"SIDEW...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>15</td>\n",
       "      <td>YOUTH IN REVOL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>6</td>\n",
       "      <td>THE DEPARTED\\n\\n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>4</td>\n",
       "      <td>SUNSHINE CLEANING...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>4</td>\n",
       "      <td>SYNECDOCHE, NEW YORK...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>19</td>\n",
       "      <td>WILD THINGS: DIAMONDS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>6</td>\n",
       "      <td>ULTIMO TANGO A ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>15</td>\n",
       "      <td>CELESTE AND JESSE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1967</th>\n",
       "      <td>6</td>\n",
       "      <td>&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;if (window!= top)\\n\\ntop.locati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968</th>\n",
       "      <td>16</td>\n",
       "      <td>TRON LEGACY\\n\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1969</th>\n",
       "      <td>15</td>\n",
       "      <td>)P(\\n\\n                                       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>4</td>\n",
       "      <td>LIAR LIAR\\n\\n  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1971</th>\n",
       "      <td>19</td>\n",
       "      <td>\"Dark Star\", short film script, by John Carpen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1972</th>\n",
       "      <td>6</td>\n",
       "      <td>BLACK\\n\\nWe HEAR \"Waltzing Matilde,\" by Tom Wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>19</td>\n",
       "      <td>SURROGATES\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974</th>\n",
       "      <td>19</td>\n",
       "      <td>CRADLE TO THE GRAVE\\n\\n             Written by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>16</td>\n",
       "      <td>\"HIGHLANDER IV: WORLD WITHOUT END\" -- Draft 9/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>19</td>\n",
       "      <td>Misery - by William Goldman\\n\\n&lt;b&gt;&lt;!--\\n\\n&lt;/b&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>11</td>\n",
       "      <td>THE HILLS HAVE ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1978 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      target                                               text\n",
       "0          8  \\t\\t\\tCrouching Tiger, Hidden Dragon\\n\\n\\t\\t\\t...\n",
       "1          4                                          \"MUMFO...\n",
       "2          6                         MAX PAYNE\\n\\n          ...\n",
       "3          6                        SLUMDOG MILLIONAIRE\\n\\n ...\n",
       "4         16  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "5         15                            THE OTHER BOLEYN GIR...\n",
       "6         19                                      GET CARTER...\n",
       "7         15                              CRAZY, STUPID, LOV...\n",
       "8          1                                      TRISTAN + ...\n",
       "9          6  BADLANDS\\n\\nby Terence Malick\\n\\nFinal Version...\n",
       "10        16                                          Rev.  ...\n",
       "11        14                    THE CURIOUS CASE OF BENJAMIN...\n",
       "12         0  \\t\\t\\t\\t\\tTHE MATRIX\\n\\n\\t\\t\\t\\t\\tWritten by\\n...\n",
       "13         6  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "14         6                                          DRIVE\\...\n",
       "15         6  \"The Truman Show\", early, by Andrew M. Niccol\\...\n",
       "16         4                                      DETROIT RO...\n",
       "17        14                      DEVIL IN A BLUE DRESS\\n\\n ...\n",
       "18         1  Spider-Man Script\\n\\n<b><!--\\n\\n</b>if (window...\n",
       "19         4                                            \"LIF...\n",
       "20         6                                              9\\...\n",
       "21         5                                  PUBLIC ENEMIES...\n",
       "22        19                                    THE ROOMMATE...\n",
       "23         6                                 MY GIRL\\n\\n    ...\n",
       "24         4                                      TWO FOR TH...\n",
       "25        19                             HEAT\\n\\n           ...\n",
       "26        19                                \"DIE HARD\"\\n\\n  ...\n",
       "27        15                       THE SHIPPING NEWS\\n\\n    ...\n",
       "28        19  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "29         2                                 MONKEYBONE\\n\\n ...\n",
       "...      ...                                                ...\n",
       "1948      15                                  MARLEY & ME\\n\\...\n",
       "1949      15                             IT'S COMPLICATED\\n\\...\n",
       "1950      14                          INCEPTION\\n\\n         ...\n",
       "1951       0  Air Force One\\n\\n<b><!--\\n\\n</b>if (window!= t...\n",
       "1952       6                              CLASH OF THE TITAN...\n",
       "1953       5  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "1954      14                                        A FEW GO...\n",
       "1955       1                              THE LORD OF THE RI...\n",
       "1956       4  \"Dark Star\", short film script, by John Carpen...\n",
       "1957       4  <b><!--\\n\\n</b>\\n\\n<b>/*\\n\\n</b>Break-out-of-f...\n",
       "1958       6                                          \"SIDEW...\n",
       "1959      15                                  YOUTH IN REVOL...\n",
       "1960       6                               THE DEPARTED\\n\\n ...\n",
       "1961       6  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "1962       4                               SUNSHINE CLEANING...\n",
       "1963       4                            SYNECDOCHE, NEW YORK...\n",
       "1964      19                           WILD THINGS: DIAMONDS...\n",
       "1965       6                                 ULTIMO TANGO A ...\n",
       "1966      15                               CELESTE AND JESSE...\n",
       "1967       6  <b><!--\\n\\n</b>if (window!= top)\\n\\ntop.locati...\n",
       "1968      16                               TRON LEGACY\\n\\n  ...\n",
       "1969      15  )P(\\n\\n                                       ...\n",
       "1970       4                                 LIAR LIAR\\n\\n  ...\n",
       "1971      19  \"Dark Star\", short film script, by John Carpen...\n",
       "1972       6  BLACK\\n\\nWe HEAR \"Waltzing Matilde,\" by Tom Wa...\n",
       "1973      19                                    SURROGATES\\n...\n",
       "1974      19  CRADLE TO THE GRAVE\\n\\n             Written by...\n",
       "1975      16  \"HIGHLANDER IV: WORLD WITHOUT END\" -- Draft 9/...\n",
       "1976      19  Misery - by William Goldman\\n\\n<b><!--\\n\\n</b>...\n",
       "1977      11                                 THE HILLS HAVE ...\n",
       "\n",
       "[1978 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"torchtext_data/train.csv\")\n",
    "df = df.rename(columns = {'Script':'text','Labels':'target'})\n",
    "#df = df[0:10]\n",
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is an example of what tokenizer does ['mumford', 'screenplay', 'by', 'lawrence', 'kasdan', 'shooting', 'draft', 'ext', '.', 'main']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer -S egment text into words, punctuations marks etc.\n",
    "\n",
    "# Was going to use spacy but I can't get it to work.\n",
    "spacy = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "tokens = tokenizer(df['text'][1])\n",
    "print('Here is an example of what tokenizer does {}'.format(tokens[0:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Basic Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete some basic text cleaning.\n",
    "# Manually removing things like punctuation.\n",
    "def normalise_text (text):\n",
    "    text = text.str.lower() # lowercase\n",
    "    text = text.str.replace(r\"\\#\",\"\") # replaces hashtags\n",
    "    text = text.str.replace(r\"http\\S+\",\"URL\")  # remove URL addresses\n",
    "    text = text.str.replace(r\"@\",\"\")\n",
    "    text = text.str.replace(r\"[^A-Za-z0-9()!?\\'\\`\\\"]\", \" \")\n",
    "    text = text.str.replace(\"\\s{2,}\", \" \")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = normalise_text(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the data into training and tes\n",
    "train_df, valid_df = train_test_split(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540    151217\n",
       "1486    122947\n",
       "1594    166672\n",
       "864     117277\n",
       "75      128172\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For each (top 5) of the entries in the train dataset how many tokens are there.\n",
    "train_df['text'].str.len().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis\n",
    "\n",
    "- How many classes\n",
    "- is there class inbalance\n",
    "- what's the size range of the different text examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 22 classes\n"
     ]
    }
   ],
   "source": [
    "num_classes = train_df['target'].nunique()\n",
    "print('There are {} classes'.format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Analysing the class imbalance')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df['target'], bins=num_classes)\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('count')\n",
    "plt.title('Analysing the class imbalance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a massive class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'How the length of the script varies')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(train_df['text'].apply(len))\n",
    "plt.xlabel('length of script')\n",
    "plt.ylabel('count')\n",
    "plt.title('How the length of the script varies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to add in sequential = True here. Look into this. \n",
    "\n",
    "# set up for using pytorch text implementations.\n",
    "TEXT = Field(tokenize=tokenizer, include_lengths = True )  # fix_length= 20000 \n",
    "LABEL = LabelField(dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pytorch dataset.\n",
    "def create_dataset(df):\n",
    "    # Do we need to add one example at a time.\n",
    "    fields = [('text',TEXT),('label', LABEL)]\n",
    "    examples_prep = []\n",
    "    for i, row in df.iterrows():\n",
    "        label = row.target\n",
    "        text = row.text\n",
    "        examples_prep.append(Example.fromlist([text,label], fields))\n",
    "\n",
    "    dataset_output = torchtext.data.Dataset(examples = examples_prep, fields=fields)\n",
    "    return dataset_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_df)\n",
    "valid_ds = create_dataset(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.example.Example at 0x23d81504358>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28999\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds[2].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings. \n",
    "You want to be able to convert each word into a vector that has some meaning. If you used one hot encoding you'd have a crazy big sparce vector for each word. Using glove 200, each word is a 200 length vector and it's been pre-set up to keep relationships between similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the text vocab. \n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train_ds, max_size= MAX_VOCAB_SIZE, vectors = 'glove.6B.200d',unk_init = torch.Tensor.zero_)\n",
    "# Why do we build vocab for the label\n",
    "LABEL.build_vocab(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an iterator\n",
    "BATCH_SIZE = 2\n",
    "train_iterator, valid_iterator = data.BucketIterator.splits(\n",
    "    (train_ds, valid_ds), \n",
    "    batch_size = BATCH_SIZE,)\n",
    "\n",
    "for batch in train_iterator:\n",
    "    text, text_lengths = batch.text\n",
    "    print(text_lengths)\n",
    "    print(text)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the text classifier\n",
    "\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "EMBED_DIM = 200\n",
    "NUN_CLASS = len(LABEL.vocab)\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_valid_loss = float('inf')\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(network.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_train_ <torchtext.data.dataset.Dataset object at 0x00000207B5C6C198>\n",
      "sub_valid_ <torchtext.data.dataset.Dataset object at 0x00000207B5C6C780>\n"
     ]
    }
   ],
   "source": [
    "sub_train_ = train_ds\n",
    "sub_valid_ = valid_ds\n",
    "\n",
    "print('sub_train_ {}'.format(sub_train_))\n",
    "print('sub_valid_ {}'.format(sub_valid_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = 0\n",
    "train_acc = 0\n",
    "data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   2, 1129],\n",
      "        [ 141, 1868],\n",
      "        [ 303,  178],\n",
      "        ...,\n",
      "        [3275,    1],\n",
      "        [   2,    1],\n",
      "        [ 261,    1]]), tensor([26738, 23858]))\n",
      "here\n",
      "tensor([1, 1])\n",
      "here\n",
      "(tensor([[ 219,   99],\n",
      "        [  27,  129],\n",
      "        [ 219, 2326],\n",
      "        ...,\n",
      "        [ 206,    1],\n",
      "        [   5,    1],\n",
      "        [ 205,    1]]), tensor([33806,  8329]))\n",
      "here\n",
      "tensor([4, 2])\n",
      "here\n",
      "(tensor([[  32],\n",
      "        [1334],\n",
      "        [2326],\n",
      "        ...,\n",
      "        [  28],\n",
      "        [   2],\n",
      "        [ 261]]), tensor([25251]))\n",
      "here\n",
      "tensor([0])\n",
      "here\n",
      "(tensor([[   25,  1114],\n",
      "        [ 5908,    65],\n",
      "        [   65, 10765],\n",
      "        ...,\n",
      "        [   28,     1],\n",
      "        [    2,     1],\n",
      "        [  261,     1]]), tensor([29175, 15695]))\n",
      "here\n",
      "tensor([3, 0])\n",
      "here\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iterator:\n",
    "    \n",
    "    for entry in batch:\n",
    "        print(entry)\n",
    "        print('here')\n",
    "    \n",
    "    \n",
    "    #print(text.shape)\n",
    "    #offsets = [0] + [len(entry) for entry in text]\n",
    "    #output = model(text)\n",
    "    #print(output.shape)\n",
    "    #print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1129],\n",
       "        [1868],\n",
       "        [ 178],\n",
       "        ...,\n",
       "        [  28],\n",
       "        [   2],\n",
       "        [ 261]])"
      ]
     },
     "execution_count": 522,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "    \n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = network(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loss, train_acc = train_func(sub_train_)\n",
    "valid_loss, valid_acc = test(sub_valid_)\n",
    "\n",
    "secs = int(time.time() - start_time)\n",
    "mins = secs / 60\n",
    "secs = secs % 60\n",
    "\n",
    "print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    #print('length {}'.format(len(batch)))\n",
    "    #print(batch.text)\n",
    "    #print(type(batch))\n",
    "    print('entry is {}'.format(batch[0]))\n",
    "    print('.text {}'.format(batch[0].text))\n",
    "    # So i think you need .text and .label to get out those. \n",
    "    print('generate_batch')\n",
    "    \n",
    "    label = torch.tensor([entry.label for entry in batch])\n",
    "    text = torch.tensor([entry.text for entry in batch])\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    #text= torch.tensor(text)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = network(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = network(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many dimensions 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-396-c7079e3340a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_train_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[0mvalid_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msub_valid_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-395-674c1575c046>\u001b[0m in \u001b[0;36mtrain_func\u001b[1;34m(sub_train_)\u001b[0m\n\u001b[0;32m      9\u001b[0m     data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n\u001b[0;32m     10\u001b[0m                       collate_fn=generate_batch)\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moffsets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-394-caeab3805b62>\u001b[0m in \u001b[0;36mgenerate_batch\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0moffsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# torch.Tensor.cumsum returns the cumulative sum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many dimensions 'str'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x207d2e611d0>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17111, 1])\n",
      "torch.Size([8329, 1])\n",
      "torch.Size([15695, 1])\n",
      "torch.Size([25251, 1])\n",
      "torch.Size([12751, 1])\n",
      "torch.Size([33806, 1])\n",
      "torch.Size([23858, 1])\n"
     ]
    }
   ],
   "source": [
    "# Each piece of text is currently a different length. \n",
    "for batch in train_iterator:\n",
    "    print((batch.text[0].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4250],\n",
      "        [ 696],\n",
      "        [1130],\n",
      "        ...,\n",
      "        [ 111],\n",
      "        [   2],\n",
      "        [ 286]])\n"
     ]
    }
   ],
   "source": [
    "# Each piece of text is currently a different length. \n",
    "for batch in train_iterator:\n",
    "    print((batch.text[0]))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x207d6aafb38>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11179\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(TEXT.vocab)\n",
    "print(vocab_size)\n",
    "embedding_dim = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to build my model\n",
    "\n",
    "class classification_net(nn.Module):\n",
    "    \n",
    "    def __init__(self,vocab_size,embedding_dim):\n",
    "        super(classification_net, self).__init__()\n",
    "        \n",
    "        #self.main = nn.Sequential(\n",
    "        #nn.Embedding(vocab_size, embedding_dim),\n",
    "        #nn.Linear(embedding_dim, 22),\n",
    "        #nn.Softmax()\n",
    "        #)\n",
    "        \n",
    "        self.em1 = nn.EmbeddingBag(vocab_size, 200)\n",
    "        self.flat = nn.Flatten(start_dim=0, end_dim=-1)\n",
    "        self.lay1 = nn.Linear(embedding_dim * 20000, 22)\n",
    "        self.soft = nn.Softmax()\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        print('input shape {}'.format(input.shape))\n",
    "        #output = self.main(input)\n",
    "        embedded = self.em1(input)\n",
    "        print('embedded shape {}'.format(embedded.shape))\n",
    "        flatten = self.flat(embedded)\n",
    "        print('flatten shape {}'.format(flatten.shape))\n",
    "        post_linear = self.lay1(flatten)\n",
    "        output = self.soft(post_linear)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = classification_net(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification_net(\n",
       "  (em1): EmbeddingBag(11179, 200, mode=mean)\n",
       "  (flat): Flatten()\n",
       "  (lay1): Linear(in_features=4000000, out_features=22, bias=True)\n",
       "  (soft): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
       "        ...,\n",
       "        [-0.4949, -0.1262, -1.1698,  ...,  0.5565,  0.5634,  0.5782],\n",
       "        [ 0.1940, -0.4843, -0.7601,  ...,  0.3863, -0.6567, -0.0112],\n",
       "        [-0.3076,  0.6030, -0.3311,  ...,  0.2374, -0.4458, -0.5565]])"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained vectors into the embedding matrix.\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "network.em1.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0715,  0.0935,  0.0237,  ...,  0.3362,  0.0306,  0.2558],\n",
      "        ...,\n",
      "        [-0.4949, -0.1262, -1.1698,  ...,  0.5565,  0.5634,  0.5782],\n",
      "        [ 0.1940, -0.4843, -0.7601,  ...,  0.3863, -0.6567, -0.0112],\n",
      "        [-0.3076,  0.6030, -0.3311,  ...,  0.2374, -0.4458, -0.5565]])\n"
     ]
    }
   ],
   "source": [
    "# What does this do?\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "#  to initiaise padded to zeros\n",
    "network.em1.weight.data[PAD_IDX] = torch.zeros(embedding_dim)\n",
    "\n",
    "print(network.em1.weight.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training function \n",
    "def train(model, iterator):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(text).squeeze(1)\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator):\n",
    "    \n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, text_lengths = batch.text\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "            \n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([20000, 1])\n",
      "embedded shape torch.Size([20000, 200])\n",
      "flatten shape torch.Size([4000000])\n",
      "tensor([0.0447, 0.0456, 0.0418, 0.0445, 0.0417, 0.0491, 0.0474, 0.0464, 0.0593,\n",
      "        0.0398, 0.0423, 0.0476, 0.0320, 0.0433, 0.0417, 0.0599, 0.0436, 0.0582,\n",
      "        0.0382, 0.0447, 0.0342, 0.0540], grad_fn=<SoftmaxBackward>)\n",
      "tensor([0.0447, 0.0456, 0.0418, 0.0445, 0.0417, 0.0491, 0.0474, 0.0464, 0.0593,\n",
      "        0.0398, 0.0423, 0.0476, 0.0320, 0.0433, 0.0417, 0.0599, 0.0436, 0.0582,\n",
      "        0.0382, 0.0447, 0.0342, 0.0540], grad_fn=<SqueezeBackward1>)\n",
      "tensor([15])\n",
      "tensor([0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalker8\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "exp_vml_cpu not implemented for 'Long'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-357-de8a3f71386e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   reduction=self.reduction)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2112\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2114\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2116\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: exp_vml_cpu not implemented for 'Long'"
     ]
    }
   ],
   "source": [
    "\n",
    "for batch in train_iterator:\n",
    "        text, text_lengths = batch.text\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        predictions = network(text)\n",
    "        predictions_squeeze = predictions.squeeze(-1)\n",
    "        print(predictions)\n",
    "        print(predictions_squeeze)\n",
    "        pred = predictions.argmax()\n",
    "        print(torch.tensor([pred]))\n",
    "        pred = torch.tensor([pred])\n",
    "        print(batch.label)\n",
    "        loss = criterion(pred, batch.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\awalker8\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Target size (torch.Size([1])) must be the same as input size (torch.Size([15695, 22]))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-39d26fccfd5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mvalid_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_iterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-c6ab8640cdfa>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, iterator)\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbinary_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    599\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    600\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m                                                   reduction=self.reduction)\n\u001b[0m\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[1;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[0;32m   2110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2111\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2112\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2114\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Target size (torch.Size([1])) must be the same as input size (torch.Size([15695, 22]))"
     ]
    }
   ],
   "source": [
    "num_epochs = 2\n",
    "t = time.time()\n",
    "loss=[]\n",
    "acc=[]\n",
    "val_acc=[]\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss, train_acc = train(network, train_iterator)\n",
    "    valid_acc = evaluate(model, valid_iterator)\n",
    "    \n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Acc: {valid_acc*100:.2f}%')\n",
    "    \n",
    "    loss.append(train_loss)\n",
    "    acc.append(train_acc)\n",
    "    val_acc.append(valid_acc)\n",
    "    \n",
    "print(f'time:{time.time()-t:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
